{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Capstone_Project.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1tzm7K11OgybQAa1MeKSG4FNlU6T0YNvy\n",
    "\n",
    "# Imports and Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import glob\n",
    "import pathlib\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dropout, Dense, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pickle as pkl\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "import requests\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "from statistics import median\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
    "    && cd keras-contrib \\\n",
    "    && python convert_to_tf_keras.py \\\n",
    "    && USE_TF_KERAS=1 python setup.py install\n",
    "clear_output()\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "drive.mount(\"/content/drive/\")\n",
    "\n",
    "!ls \"/content/drive/My Drive/Capstone/anime/animated2\"\n",
    "\n",
    "\"\"\"# Anger DataFrame\"\"\"\n",
    "\n",
    "anger = glob.glob('/content/drive/MyDrive/Capstone/anime/animated2/cartoon_anger/*.png')\n",
    "anger_names = [str(i)[::-1].split(\"/\", 1)[0][::-1] for i in anger]\n",
    "anger_directories = [str(i)[::-1].split(\"/\", 1)[1][::-1] for i in anger]\n",
    "cartoon_anger = pd.DataFrame()\n",
    "cartoon_anger[\"Directories\"], cartoon_anger[\"Names\"], cartoon_anger[\"Emotion\"], cartoon_anger[\"Y\"] = anger_directories, anger_names, \"anger\", 0\n",
    "\n",
    "cartoon_anger\n",
    "\n",
    "img = cv2.imread(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_anger/aia_anger_1.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "\"\"\"# Joy DataFrame\"\"\"\n",
    "\n",
    "joy = glob.glob('/content/drive/MyDrive/Capstone/anime/animated2/cartoon_joy/*.png')\n",
    "joy_names = [str(i)[::-1].split(\"/\", 1)[0][::-1] for i in joy]\n",
    "joy_directories = [str(i)[::-1].split(\"/\", 1)[1][::-1] for i in joy]\n",
    "cartoon_joy = pd.DataFrame()\n",
    "cartoon_joy[\"Directories\"], cartoon_joy[\"Names\"], cartoon_joy[\"Emotion\"], cartoon_joy[\"Y\"] = joy_directories, joy_names, \"joy\", 1\n",
    "\n",
    "cartoon_joy\n",
    "\n",
    "img = cv2.imread(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_joy/jules_joy_1.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "\"\"\"# Surprise DataFrame\"\"\"\n",
    "\n",
    "surprise = glob.glob('/content/drive/MyDrive/Capstone/anime/animated2/cartoon_surprise/*.png')\n",
    "surprise_names = [str(i)[::-1].split(\"/\", 1)[0][::-1] for i in surprise]\n",
    "surprise_directories = [str(i)[::-1].split(\"/\", 1)[1][::-1] for i in surprise]\n",
    "cartoon_surprise = pd.DataFrame()\n",
    "cartoon_surprise[\"Directories\"], cartoon_surprise[\"Names\"], cartoon_surprise[\"Emotion\"], cartoon_surprise[\"Y\"] = surprise_directories, surprise_names, \"surprise\", 2\n",
    "\n",
    "cartoon_surprise\n",
    "\n",
    "img = cv2.imread(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_surprise/malcolm_surprise_1.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "\"\"\"# Disgust DataFrame\"\"\"\n",
    "\n",
    "disgust = glob.glob('/content/drive/MyDrive/Capstone/anime/animated2/cartoon_disgust/*.png')\n",
    "disgust_names = [str(i)[::-1].split(\"/\", 1)[0][::-1] for i in disgust]\n",
    "disgust_directories = [str(i)[::-1].split(\"/\", 1)[1][::-1] for i in disgust]\n",
    "cartoon_disgust = pd.DataFrame()\n",
    "cartoon_disgust[\"Directories\"], cartoon_disgust[\"Names\"], cartoon_disgust[\"Emotion\"], cartoon_disgust[\"Y\"] = disgust_directories, disgust_names, \"disgust\", 3\n",
    "\n",
    "cartoon_disgust\n",
    "\n",
    "img = cv2.imread(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_disgust/bonnie_disgust_1.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "\"\"\"# Fear DataFrame\"\"\"\n",
    "\n",
    "fear = glob.glob('/content/drive/MyDrive/Capstone/anime/animated2/cartoon_fear/*.png')\n",
    "fear_names= [str(i)[::-1].split(\"/\", 1)[0][::-1] for i in fear]\n",
    "fear_directories = [str(i)[::-1].split(\"/\", 1)[1][::-1] for i in fear]\n",
    "cartoon_fear = pd.DataFrame()\n",
    "cartoon_fear[\"Directories\"], cartoon_fear[\"Names\"], cartoon_fear[\"Emotion\"], cartoon_fear[\"Y\"] = fear_directories, fear_names, \"fear\", 4\n",
    "\n",
    "cartoon_fear\n",
    "\n",
    "img = cv2.imread(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_fear/mery_fear_1.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "\"\"\"# Sadness DataFrame\"\"\"\n",
    "\n",
    "sadness = glob.glob('/content/drive/MyDrive/Capstone/anime/animated2/cartoon_sadness/*.png')\n",
    "sadness_names = [str(i)[::-1].split(\"/\", 1)[0][::-1] for i in sadness]\n",
    "sadness_directories = [str(i)[::-1].split(\"/\", 1)[1][::-1] for i in sadness]\n",
    "cartoon_sadness = pd.DataFrame()\n",
    "cartoon_sadness[\"Directories\"], cartoon_sadness[\"Names\"], cartoon_sadness[\"Emotion\"], cartoon_sadness[\"Y\"] = sadness_directories, sadness_names, \"sadness\", 5\n",
    "\n",
    "cartoon_sadness\n",
    "\n",
    "img = cv2.imread(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_sadness/ray_sadness_1.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "\"\"\"# Neutral DataFrame\"\"\"\n",
    "\n",
    "neutral = glob.glob('/content/drive/MyDrive/Capstone/anime/animated2/cartoon_neutral/*.png')\n",
    "neutral_names = [str(i)[::-1].split(\"/\", 1)[0][::-1] for i in neutral]\n",
    "neutral_directories = [str(i)[::-1].split(\"/\", 1)[1][::-1] for i in neutral]\n",
    "cartoon_neutral = pd.DataFrame()\n",
    "cartoon_neutral[\"Directories\"], cartoon_neutral[\"Names\"], cartoon_neutral[\"Emotion\"], cartoon_neutral[\"Y\"] = neutral_directories, neutral_names, \"neutral\", 6\n",
    "\n",
    "cartoon_neutral\n",
    "\n",
    "img = cv2.imread(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_neutral/jules_neutral_1.png\")\n",
    "plt.imshow(img)\n",
    "\n",
    "\"\"\"# Concatenating all dataframes -> Cartoon DataFrame\"\"\"\n",
    "\n",
    "cartoon = pd.concat([cartoon_anger, cartoon_joy, cartoon_surprise, cartoon_disgust, cartoon_fear, cartoon_sadness, cartoon_neutral])\n",
    "\n",
    "cartoon\n",
    "\n",
    "#Train test split\n",
    "cartoon_train, cartoon_test = train_test_split(cartoon, stratify=cartoon[\"Y\"], test_size = 0.2)\n",
    "\n",
    "cartoon_train\n",
    "\n",
    "\"\"\"# Loading Saved DataFrames\"\"\"\n",
    "\n",
    "# cartoon_train.to_csv(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_train.csv\")\n",
    "# cartoon_test.to_csv(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_test.csv\")\n",
    "\n",
    "cartoon_train = pd.read_csv(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_train.csv\")\n",
    "cartoon_test = pd.read_csv(\"/content/drive/MyDrive/Capstone/anime/animated2/cartoon_test.csv\")\n",
    "\n",
    "cartoon_train[\"Y\"].value_counts()\n",
    "\n",
    "df = cartoon_train.sample(frac = 1)\n",
    "df = df.reset_index()\n",
    "df\n",
    "\n",
    "\"\"\"# Model Configuration and Training\"\"\"\n",
    "\n",
    "def get_models():\n",
    "  model_bottleneck = VGG16(weights='imagenet', include_top=False)\n",
    "  model_cartoon = load_model(\"/content/drive/MyDrive/Capstone/anime/animated2/ResizedModelSingleEpoch.h5\")\n",
    "  model_cartoon.compile(loss='categorical_crossentropy', optimizer='Adam', metrics = ['accuracy'])\n",
    "  \n",
    "  return model_bottleneck, model_cartoon\n",
    "\n",
    "# Transfer learning -> Using pretrained VGG with imagenet weights. Fine tuning to be done with cartoon images\n",
    "\n",
    "batch_size = 20 # Stream and train batches of 20 images at once. Cannot load entire dataset into limited RAM\n",
    "ishape = 8*8*512 # Shape of VGG output\n",
    "\n",
    "# Train model for 1 epoch. Time taken ~ 3 hours\n",
    "\n",
    "modelvgg = Sequential()\n",
    "modelvgg.add(Flatten())\n",
    "modelvgg.add(Dense(512, activation='relu', input_shape = (ishape, )))\n",
    "modelvgg.add(Dropout(0.2))\n",
    "modelvgg.add(Dense(256, activation='relu'))\n",
    "modelvgg.add(Dense(128, activation='relu'))\n",
    "modelvgg.add(BatchNormalization())\n",
    "modelvgg.add(Dense(64, activation='relu'))\n",
    "modelvgg.add(Dense(7, activation='softmax')) \n",
    "modelvgg.compile(loss='categorical_crossentropy', optimizer='Adam', metrics = ['accuracy'])\n",
    "\n",
    "# for i in range(int(len(df)/batch_size)):\n",
    "#   try:\n",
    "#     print(\"Training on batch \" +str(i+1))\n",
    "#     temp_df = df[i*batch_size:(i+1)*batch_size]\n",
    "#     images, labels = get_cartoon_images_and_labels(temp_df)\n",
    "#     images = convert_cartoon_grayscale(resize_cartoon_images(images))\n",
    "#     predictions = model.predict(images)\n",
    "#     modelvgg.train_on_batch(np.asarray(predictions), labels)\n",
    "#     del images\n",
    "#     del labels\n",
    "#     del temp_df\n",
    "#   except:\n",
    "#     print(\"Error on batch \" + str(i+1))\n",
    "\n",
    "# Accuracy here is 100%, but our end goal is to obtain a better accuracy on human images.\n",
    "# modelvgg.save(\"/content/drive/MyDrive/Capstone/anime/animated2/ResizedModelSingleEpoch.h5\")\n",
    "# Load saved model from drive and test accuracy = 100%\n",
    "\n",
    "\"\"\"# Image preprocessing\"\"\"\n",
    "\n",
    "def get_human_paths():\n",
    "  human_anger_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/CK+48/anger/*.png\")\n",
    "  df0 = pd.DataFrame({'Path':human_anger_paths, 'Label':[0 for i in human_anger_paths]})\n",
    "  human_disgust_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/CK+48/disgust/*.png\")\n",
    "  df3 = pd.DataFrame({'Path':human_disgust_paths, 'Label':[3 for i in human_disgust_paths]})\n",
    "  human_fear_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/CK+48/fear/*.png\")\n",
    "  df4 = pd.DataFrame({'Path':human_fear_paths, 'Label':[4 for i in human_fear_paths]})\n",
    "  human_sadness_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/CK+48/sadness/*.png\")\n",
    "  df5 = pd.DataFrame({'Path':human_sadness_paths, 'Label':[5 for i in human_sadness_paths]})\n",
    "  human_joy_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/CK+48/happy/*.png\")\n",
    "  df1 = pd.DataFrame({'Path':human_joy_paths, 'Label':[1 for i in human_joy_paths]})\n",
    "  human_surprise_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/CK+48/surprise/*.png\")\n",
    "  df2 = pd.DataFrame({'Path':human_surprise_paths, 'Label':[2 for i in human_surprise_paths]})\n",
    "  human_paths = human_anger_paths + human_disgust_paths + human_fear_paths + human_joy_paths + human_sadness_paths + human_surprise_paths\n",
    "  human_df = pd.concat([df0, df1, df2, df3, df4, df5]).reset_index()\n",
    "\n",
    "  return human_paths, human_df\n",
    "\n",
    "def get_cartoon_images_and_labels(temp_df, integer_labels = False):\n",
    "  paths = [i+\"/\"+j for i,j in zip(temp_df[\"Directories\"].tolist(), temp_df[\"Names\"].tolist())]\n",
    "  images = np.asarray([np.asarray(Image.open(image))[:, :, :3] for image in paths])\n",
    "  labels = np.asarray(temp_df[\"Y\"].tolist())\n",
    "  labels = to_categorical(labels, num_classes=7)\n",
    "  integerLabels = np.asarray([np.argmax(label) for label in labels])\n",
    "\n",
    "  if (integer_labels):\n",
    "    return images, labels, integerLabels\n",
    "  else:\n",
    "    return images, labels\n",
    "\n",
    "def resize_cartoon_images(images):\n",
    "  clipped_images = [image[40:240, 35:225] for image in images]\n",
    "  resized_clipped_images = [cv2.resize(clipped_image, (256, 256)) for clipped_image in clipped_images]\n",
    "  return np.asarray(resized_clipped_images)\n",
    "\n",
    "def convert_cartoon_grayscale(images):\n",
    "  return np.asarray([np.asarray(ImageOps.grayscale(Image.fromarray(image)).convert(\"RGB\")) for image in images])\n",
    "\n",
    "def display_images(images):\n",
    "  for image in images:\n",
    "    display(Image.fromarray(image))\n",
    "    print()\n",
    "\n",
    "def get_human_cartoonized_images(human_ct_df, integer_labels = False, augment = False):\n",
    "  paths = human_ct_df[\"Path\"]\n",
    "  if (augment):\n",
    "    human_ct_images = np.concatenate((np.asarray([np.fliplr(np.asarray(ImageOps.grayscale(Image.open(path)).convert(\"RGB\"))) for path in paths]), np.asarray([np.asarray(ImageOps.grayscale(Image.open(path)).convert(\"RGB\")) for path in paths])), axis = 0)\n",
    "    human_ct_labels = np.concatenate((np.asarray(human_ct_df[\"Label\"].tolist()), np.asarray(human_ct_df[\"Label\"].tolist())), axis = 0)\n",
    "    human_ct_labels = np.asarray(to_categorical(human_ct_labels, num_classes=7))\n",
    "  else:\n",
    "    human_ct_images = np.asarray([np.asarray(ImageOps.grayscale(Image.open(path)).convert(\"RGB\")) for path in paths])\n",
    "    human_ct_labels = np.asarray(human_ct_df[\"Label\"].tolist())\n",
    "    human_ct_labels = np.asarray(to_categorical(human_ct_labels, num_classes=7))\n",
    "\n",
    "  integerLabels = np.asarray([np.argmax(label) for label in human_ct_labels])\n",
    "\n",
    "  if (integer_labels):\n",
    "    return human_ct_images, human_ct_labels, integerLabels\n",
    "  else:\n",
    "    return human_ct_images, human_ct_labels\n",
    "\n",
    "def get_human_images(human_df, integer_labels = False):\n",
    "  paths = human_df[\"Path\"]\n",
    "  human_images = np.asarray([cv2.resize(np.asarray(Image.open(path).convert(\"RGB\")), (256, 256)) for path in paths])\n",
    "  human_labels = np.asarray(human_df[\"Label\"].tolist())\n",
    "  human_labels = np.asarray(to_categorical(human_labels, num_classes=7))\n",
    "  integerLabels = np.asarray([np.argmax(label) for label in human_labels])\n",
    "\n",
    "  if (integer_labels):\n",
    "    return human_images, human_labels, integerLabels\n",
    "  else:\n",
    "    return human_images, human_labels\n",
    "\n",
    "def train_human(model_bottleneck, model_cartoon, human_images, human_labels, epochs, lr = 0.1):\n",
    "  human_bottleneck = np.asarray(model_bottleneck.predict(human_images))\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    #print(\"Training epoch \" + str(epoch+1))\n",
    "    for i in range(len(human_bottleneck)//20):\n",
    "      model_cartoon.train_on_batch(human_bottleneck[20*i:20*(i+1)], human_labels[20*i:20*(i+1)])\n",
    "  \n",
    "  return model_cartoon\n",
    "\n",
    "def get_human_cartoonized_paths():\n",
    "  human_anger_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/shinkai/0*.png\")\n",
    "  df0 = pd.DataFrame({'Path':human_anger_paths, 'Label':[0 for i in human_anger_paths]})\n",
    "  human_disgust_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/shinkai/3*.png\")\n",
    "  df3 = pd.DataFrame({'Path':human_disgust_paths, 'Label':[3 for i in human_disgust_paths]})\n",
    "  human_fear_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/shinkai/4*.png\")\n",
    "  df4 = pd.DataFrame({'Path':human_fear_paths, 'Label':[4 for i in human_fear_paths]})\n",
    "  human_sadness_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/shinkai/5*.png\")\n",
    "  df5 = pd.DataFrame({'Path':human_sadness_paths, 'Label':[5 for i in human_sadness_paths]})\n",
    "  human_joy_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/shinkai/1*.png\")\n",
    "  df1 = pd.DataFrame({'Path':human_joy_paths, 'Label':[1 for i in human_joy_paths]})\n",
    "  human_surprise_paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/shinkai/2*.png\")\n",
    "  df2 = pd.DataFrame({'Path':human_surprise_paths, 'Label':[2 for i in human_surprise_paths]})\n",
    "  human_paths = human_anger_paths + human_disgust_paths + human_fear_paths + human_joy_paths + human_sadness_paths + human_surprise_paths\n",
    "  human_df = pd.concat([df0, df1, df2, df3, df4, df5]).reset_index()\n",
    "\n",
    "  return human_paths, human_df\n",
    "\n",
    "\"\"\"# Accuracy Testing\"\"\"\n",
    "\n",
    "def test_cartoon_accuracy(model_bottleneck, model_test, temp_df, display_image = True):\n",
    "  temp_df = temp_df.reset_index()\n",
    "  images, labels = get_cartoon_images_and_labels(temp_df)\n",
    "  images = convert_cartoon_grayscale(resize_cartoon_images(images))\n",
    "  bottleneck_features = model_bottleneck.predict(images)\n",
    "  predictions = model_test.predict_classes(bottleneck_features)\n",
    "  probabilities = model_test.predict(bottleneck_features)\n",
    "\n",
    "  key = {0:\"anger\", 1:\"joy\", 2:\"surprise\", 3:\"disgust\", 4:\"fear\", 5:\"sadness\", 6:\"neutral\"}\n",
    "  integerLabels = np.asarray([np.argmax(label) for label in labels])\n",
    "\n",
    "  for index, image in list(enumerate(images)):\n",
    "    if(display_image):\n",
    "      display(Image.fromarray(image))\n",
    "      print()\n",
    "      print(temp_df[\"Names\"][index])\n",
    "      print(\"Predicted Class: \" + str(key[predictions[index]]))\n",
    "      print(\"Confidence in Prediction: \" + str(np.max(probabilities[index])))\n",
    "      print(\"Actual Class: \" + str(key[integerLabels[index]]))\n",
    "      print(\"__________________________________________________\\n\")\n",
    "\n",
    "  accuracy = accuracy_score(integerLabels, predictions)\n",
    "  metrics = precision_recall_fscore_support(integerLabels, predictions, average='weighted')\n",
    "  confusion = confusion_matrix(integerLabels, predictions)\n",
    "\n",
    "  return predictions, probabilities, accuracy, metrics, confusion\n",
    "\n",
    "def test_human_accuracy(model_bottleneck, model_test, human_df, display_image = True):\n",
    "  human_df = human_df.reset_index()\n",
    "  human_images, human_labels = get_human_images(human_df)\n",
    "  test_human_bottleneck =  np.asarray(model_bottleneck.predict(human_images))\n",
    "  predictions = model_test.predict_classes(test_human_bottleneck)\n",
    "  probabilities = model_test.predict(test_human_bottleneck)\n",
    "\n",
    "  key = {0:\"anger\", 1:\"joy\", 2:\"surprise\", 3:\"disgust\", 4:\"fear\", 5:\"sadness\", 6:\"neutral\"}\n",
    "  integerLabels = np.asarray([np.argmax(label) for label in human_labels])\n",
    "\n",
    "  for index, image in list(enumerate(human_images)):\n",
    "    if(display_image):\n",
    "      display(Image.fromarray(image))\n",
    "      print()\n",
    "      print(\"Predicted Class: \" + str(key[predictions[index]]))\n",
    "      print(\"Confidence in Prediction: \" + str(np.max(probabilities[index])))\n",
    "      print(\"Actual Class: \" + str(key[integerLabels[index]]))\n",
    "      print(\"__________________________________________________\\n\")\n",
    "\n",
    "  accuracy = accuracy_score(integerLabels, predictions)\n",
    "  metrics = precision_recall_fscore_support(integerLabels, predictions, average='weighted')\n",
    "  confusion = confusion_matrix(integerLabels, predictions)\n",
    "\n",
    "  return predictions, probabilities, accuracy, metrics, confusion\n",
    "\n",
    "def test_human_cartoonized_accuracy(model_bottleneck, model_test, human_ct_df, display_image = True, augment = False):\n",
    "  human_ct_df = human_ct_df.reset_index()\n",
    "  if (augment):\n",
    "    human_ct_images, human_ct_labels = get_human_cartoonized_images(human_ct_df, augment = True)\n",
    "  else:\n",
    "    human_ct_images, human_ct_labels = get_human_cartoonized_images(human_ct_df)\n",
    "  test_human_bottleneck =  np.asarray(model_bottleneck.predict(human_ct_images))\n",
    "  predictions = model_test.predict_classes(test_human_bottleneck)\n",
    "  probabilities = model_test.predict(test_human_bottleneck)\n",
    "\n",
    "  key = {0:\"anger\", 1:\"joy\", 2:\"surprise\", 3:\"disgust\", 4:\"fear\", 5:\"sadness\", 6:\"neutral\"}\n",
    "  integerLabels = np.asarray([np.argmax(label) for label in human_ct_labels])\n",
    "\n",
    "  for index, image in list(enumerate(human_ct_images)):\n",
    "    if(display_image):\n",
    "      display(Image.fromarray(image))\n",
    "      print()\n",
    "      print(\"Predicted Class: \" + str(key[predictions[index]]))\n",
    "      print(\"Confidence in Prediction: \" + str(np.max(probabilities[index])))\n",
    "      print(\"Actual Class: \" + str(key[integerLabels[index]]))\n",
    "      print(\"__________________________________________________\\n\")\n",
    "\n",
    "  accuracy = accuracy_score(integerLabels, predictions)\n",
    "  metrics = precision_recall_fscore_support(integerLabels, predictions, average='weighted')\n",
    "  confusion = confusion_matrix(integerLabels, predictions)\n",
    "\n",
    "  return predictions, probabilities, accuracy, metrics, confusion\n",
    "\n",
    "\"\"\"# Future Work\"\"\"\n",
    "\n",
    "# Dataset Used - FERG-DB\n",
    "\n",
    "# To do Tasks:\n",
    "# 1) Implement K-fold cross validation\n",
    "# 2) Cartoonize human images\n",
    "# 3) Test Model on cartoonized human images\n",
    "# 4) Enable live face tracking and cartoonization using openCV\n",
    "# 5) Test effects of image augmentation\n",
    "#       - Geographic Transformations\n",
    "#       - Flipping sideways\n",
    "#       - Colour Transformations\n",
    "#       - Cropping measures\n",
    "# 6) Track how model performs on human images at different levels of available data (10 images, 100 images, 1000 images, complete dataset, etc.)\n",
    "# 7) Experiment with other bottleneck feature extractors than the VGG - Residual Networks, Inception Networks, DenseNets, etc.\n",
    "\n",
    "# 17 - train with cropped faces with animated and then with human\n",
    "# 18 - use basic cartoonization on human images and test accuracies\n",
    "# 19, 20 - get final accuracies somehow and begin research paper\n",
    "# 21, 22 - implement opencv functionality\n",
    "\n",
    "# 25 - cartoonize cohn kanade dataset with cgan\n",
    "# 26 - Write functions to implement kfold, get accuracies for different data percentages, different augmentation techniques(flip image), and number of epochs\n",
    "# 27 - complete base research paper\n",
    "# 28 - format final research paper\n",
    "# 29 - opencv implementation\n",
    "# 30 -\n",
    "\n",
    "\"\"\"# Experimental\"\"\"\n",
    "\n",
    "test_df = cartoon_test.sample(frac = 0.01)\n",
    "\n",
    "test_df.shape\n",
    "\n",
    "model_bottleneck, model_cartoon = get_models()\n",
    "\n",
    "predictions, probabilities, accuracy, metrics, confusion = test_cartoon_accuracy(model_bottleneck, model_cartoon, test_df, display_image = False)\n",
    "\n",
    "metrics\n",
    "\n",
    "accuracy\n",
    "\n",
    "human_paths, human_df = get_human_paths()\n",
    "\n",
    "human_images, human_labels = get_human_images(human_df)\n",
    "human_images_ct, human_labels_ct = get_human_cartoonized_images(human_ct_df)\n",
    "\n",
    "human_train, human_test = train_test_split(human_df, stratify = human_df[\"Label\"], test_size = 0.2)\n",
    "\n",
    "human_images_train, human_labels_train = get_human_images(human_train)\n",
    "human_images_test, human_labels_test = get_human_images(human_test)\n",
    "\n",
    "model_cartoon = train_human(model_bottleneck, model_cartoon, human_images_train, human_labels_train, epochs = 10)\n",
    "\n",
    "human_predictions, human_probabilities, human_accuracy, human_metrics, human_confusion = test_human_accuracy(model_bottleneck, model_cartoon, human_test)\n",
    "\n",
    "human_accuracy\n",
    "\n",
    "human_ct_paths, human_ct_df = get_human_cartoonized_paths()\n",
    "\n",
    "im1 = cv2.imread(human_ct_paths[0])\n",
    "\n",
    "im1.shape\n",
    "\n",
    "np.asarray(ImageOps.grayscale(Image.fromarray(im1))).shape\n",
    "\n",
    "human_ct_train, human_ct_test = train_test_split(human_ct_df, stratify = human_ct_df[\"Label\"], test_size = 0.3)\n",
    "\n",
    "human_images_ct_train, human_labels_ct_train = get_human_cartoonized_images(human_ct_train)\n",
    "human_images_ct_test, human_labels_ct_test = get_human_cartoonized_images(human_ct_test)\n",
    "\n",
    "model_cartoon = train_human(model_bottleneck, model_cartoon, human_images_ct_train, human_labels_ct_train, epochs = 50)\n",
    "\n",
    "human_ct_predictions, human_ct_probabilities, human_ct_accuracy, human_ct_metrics, human_ct_confusion = test_human_cartoonized_accuracy(model_bottleneck, model_cartoon, human_ct_test, display_image=False)\n",
    "\n",
    "human_ct_accuracy\n",
    "\n",
    "# get kfold splits\n",
    "# loop over number of epochs\n",
    "# train model on data\n",
    "# track accuracy, precision, recall, f1 on cartoonized images\n",
    "\n",
    "human_images_ct, human_labels_ct, human_labels_ct_integers = get_human_cartoonized_images(human_ct_df, integer_labels = True)\n",
    "\n",
    "\"\"\"# Data Collection\"\"\"\n",
    "\n",
    "human_ct_paths, human_ct_df = get_human_cartoonized_paths()\n",
    "\n",
    "class Data:\n",
    "  percentage = -1\n",
    "  epoch = -1\n",
    "  fold_count = -1\n",
    "  test_predictions = -1\n",
    "  test_probabilities = -1\n",
    "  test_accuracy = -1\n",
    "  test_metrics = -1\n",
    "  test_confusion = -1\n",
    "  train_predictions = -1\n",
    "  train_probabilities = -1\n",
    "  train_accuracy = -1\n",
    "  train_metrics = -1\n",
    "  train_confusion = -1\n",
    "\n",
    "def train_kfold(human_ct_df, epochs, percentages):\n",
    "  data_list = []\n",
    "  for percentage in percentages:\n",
    "    if (percentage!=100):\n",
    "      trainer, tester = train_test_split(human_ct_df, stratify = human_ct_df[\"Label\"], test_size = 1-percentage/100)\n",
    "    else:\n",
    "      trainer = human_ct_df\n",
    "      tester = pd.DataFrame(columns = human_ct_df.columns)\n",
    "    \n",
    "    skfold = StratifiedKFold(n_splits = 10, shuffle = (percentage == 100))\n",
    "    human_images_ct, human_labels_ct, human_labels_ct_integers = get_human_cartoonized_images(trainer, integer_labels = True)\n",
    "    fold_count = 0\n",
    "\n",
    "    for train_index, test_index in skfold.split(human_images_ct, human_labels_ct_integers):\n",
    "      train_images, test_images = human_images_ct[train_index], human_images_ct[test_index]\n",
    "      train_labels, test_labels = human_labels_ct[train_index], human_labels_ct[test_index]\n",
    "      model_bottleneck, model_cartoon = get_models()\n",
    "\n",
    "      for epoch in range(epochs):\n",
    "        print(\"Training Percentage: \" + str(percentage) + \" | Fold: \" + str(fold_count) + \" | Epoch :\" + str(epoch))\n",
    "        data_obj = Data()\n",
    "        model_cartoon = train_human(model_bottleneck, model_cartoon, train_images, train_labels, epochs = 1)\n",
    "        test_predictions, test_probabilities, test_accuracy, test_metrics, test_confusion = test_human_cartoonized_accuracy(model_bottleneck, model_cartoon, pd.concat([trainer.iloc[test_index], tester]), display_image = False)\n",
    "        train_predictions, train_probabilities, train_accuracy, train_metrics, train_confusion = test_human_cartoonized_accuracy(model_bottleneck, model_cartoon, trainer.iloc[train_index], display_image = False)\n",
    "        data_obj.percentage = percentage\n",
    "        data_obj.epoch = epoch\n",
    "        data_obj.fold_count = fold_count\n",
    "        data_obj.test_predictions = test_predictions\n",
    "        data_obj.test_probabilities = test_probabilities\n",
    "        data_obj.test_accuracy = test_accuracy\n",
    "        data_obj.test_metrics = test_metrics\n",
    "        data_obj.test_confusion = test_confusion\n",
    "        data_obj.train_predictions = train_predictions\n",
    "        data_obj.train_probabilities = train_probabilities\n",
    "        data_obj.train_accuracy = train_accuracy\n",
    "        data_obj.train_metrics = train_metrics\n",
    "        data_obj.train_confusion = train_confusion\n",
    "\n",
    "        data_list.append(data_obj)\n",
    "\n",
    "        collected = gc.collect()\n",
    "\n",
    "      fold_count+=1\n",
    "\n",
    "  return data_list\n",
    "\n",
    "def train_kfold_augmented(human_ct_df, epochs, percentages):\n",
    "  data_list = []\n",
    "  for percentage in percentages:\n",
    "    if (percentage!=100):\n",
    "      trainer, tester = train_test_split(human_ct_df, stratify = human_ct_df[\"Label\"], test_size = 1-percentage/100)\n",
    "      print(\"not 100\")\n",
    "    else:\n",
    "      trainer = human_ct_df.sample(frac = 1).reset_index(drop = True)\n",
    "      tester = pd.DataFrame(columns = human_ct_df.columns)\n",
    "      print(\"100\")\n",
    "    print(percentage==100)\n",
    "    skfold = StratifiedKFold(n_splits = 10, shuffle = (percentage == 100))\n",
    "\n",
    "    fold_count = 0\n",
    "\n",
    "    for train_index, test_index in skfold.split(trainer, trainer[\"Label\"]):\n",
    "      train_images, train_labels = get_human_cartoonized_images(trainer.iloc[train_index], augment = True)\n",
    "      test_images, test_labels = get_human_cartoonized_images(trainer.iloc[test_index], augment = True)\n",
    "      model_bottleneck, model_cartoon = get_models()\n",
    "\n",
    "      for epoch in range(epochs):\n",
    "        print(\"Training Percentage: \" + str(percentage) + \" | Fold: \" + str(fold_count) + \" | Epoch: \" + str(epoch))\n",
    "        data_obj = Data()\n",
    "        model_cartoon = train_human(model_bottleneck, model_cartoon, train_images, train_labels, epochs = 1)\n",
    "        test_predictions, test_probabilities, test_accuracy, test_metrics, test_confusion = test_human_cartoonized_accuracy(model_bottleneck, model_cartoon, pd.concat([trainer.iloc[test_index], tester]), display_image = False, augment = True)\n",
    "        train_predictions, train_probabilities, train_accuracy, train_metrics, train_confusion = test_human_cartoonized_accuracy(model_bottleneck, model_cartoon, trainer.iloc[train_index], display_image = False, augment = True)\n",
    "        \n",
    "        data_obj.percentage = percentage\n",
    "        data_obj.epoch = epoch\n",
    "        data_obj.fold_count = fold_count\n",
    "        data_obj.test_predictions = test_predictions\n",
    "        data_obj.test_probabilities = test_probabilities\n",
    "        data_obj.test_accuracy = test_accuracy\n",
    "        data_obj.test_metrics = test_metrics\n",
    "        data_obj.test_confusion = test_confusion\n",
    "        data_obj.train_predictions = train_predictions\n",
    "        data_obj.train_probabilities = train_probabilities\n",
    "        data_obj.train_accuracy = train_accuracy\n",
    "        data_obj.train_metrics = train_metrics\n",
    "        data_obj.train_confusion = train_confusion\n",
    "\n",
    "        print(test_accuracy)\n",
    "        print(train_accuracy)\n",
    "\n",
    "        data_list.append(data_obj)\n",
    "\n",
    "        collected = gc.collect()\n",
    "\n",
    "      fold_count+=1\n",
    "      \n",
    "\n",
    "  return data_list\n",
    "\n",
    "# for i in [100]:\n",
    "#   data_augmented_newer = train_kfold_augmented(human_ct_df, epochs = 10, percentages=[i])\n",
    "  # file_name = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_augmented_new\" + str(i) + \".pkl\"\n",
    "  # open_file = open(file_name, \"wb\")\n",
    "  # pickle.dump(data_augmented, open_file)\n",
    "  # open_file.close()\n",
    "\n",
    "# file_name = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_augmented_newer.pkl\"\n",
    "# open_file = open(file_name, \"wb\")\n",
    "# pickle.dump(data_augmented_newer, open_file)\n",
    "# open_file.close()\n",
    "\n",
    "# for i in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "#   data_augmented = train_kfold(human_ct_df, epochs = 10, percentages=[i])\n",
    "#   file_name = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_unaugmented_new\" + str(i) + \".pkl\"\n",
    "#   open_file = open(file_name, \"wb\")\n",
    "#   pickle.dump(data_augmented, open_file)\n",
    "#   open_file.close()\n",
    "\n",
    "# file_name = \"data_90_100.pkl\"\n",
    "# open_file = open(file_name, \"wb\")\n",
    "# pickle.dump(data, open_file)\n",
    "# open_file.close()\n",
    "# files.download(file_name)\n",
    "\n",
    "# data_unaugmented = []\n",
    "# for i in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "#   file_name = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_unaugmented_new\" + str(i) + \".pkl\"\n",
    "#   with open(file_name, 'rb') as f:\n",
    "#     data_unaugmented += pickle.load(f)\n",
    "#   open_file.close()\n",
    "\n",
    "file_name1 = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_augmented_final.pkl\"\n",
    "file_name2 = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_unaugmented_final.pkl\"\n",
    "file_name3 = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_unaugmented_newer.pkl\"\n",
    "file_name4 = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_augmented_newer.pkl\"\n",
    "\n",
    "with open(file_name1, 'rb') as f:\n",
    "  data_augmented = pickle.load(f)\n",
    "with open(file_name2, 'rb') as f:\n",
    "  data_unaugmented = pickle.load(f)\n",
    "with open(file_name3, 'rb') as f:\n",
    "  data_unaugmented_newer = pickle.load(f)\n",
    "with open(file_name4, 'rb') as f:\n",
    "  data_augmented_newer = pickle.load(f)\n",
    "\n",
    "data_augmented[900:1000] = data_augmented_newer\n",
    "data_unaugmented[900:1000] = data_unaugmented_newer\n",
    "\n",
    "# 27 - # Add image augmentation stuff, get all data\n",
    "# 28 - # Plot all graphs, re-write the abstract, intro and methodologies\n",
    "# 28 - # Write the pre-processing, dataset used, etc. sections\n",
    "# 28 - # Compile into google doc document\n",
    "# 29 - # Make IEEE Format\n",
    "\n",
    "# filename = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_augmented.pkl\"\n",
    "# with open(filename, 'rb') as f:\n",
    "#     data_augmented = pickle.load(f)\n",
    "\n",
    "# filename = \"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/data_unaugmented.pkl\"\n",
    "# with open(filename, 'rb') as f:\n",
    "#     data_unaugmented = pickle.load(f)\n",
    "\n",
    "list(data_augmented[0].__dict__.keys())\n",
    "\n",
    "# Plots:\n",
    "# For 10 epochs, 10-fold median training and testing accuracies for augmented and unaugmented for each percentage of data [total 10 plots]\n",
    "# Make dataframe with fold median training, testing accuracies, together with different labels for augmented and unaugmented for each percentage sepearately for each epoch\n",
    "\n",
    "columns_list = [\"Epoch\", \"Augmented_Training_Accuracy\", \"Augmented_Testing_Accuracy\", \"Augmented_Testing_FScore\", \"Unaugmented_Training_Accuracy\", \"Unaugmented_Testing_Accuracy\", \"Unaugmented_Testing_FScore\"]\n",
    "\n",
    "def get_dataframe(df_percentage):\n",
    "  data_df = pd.DataFrame(columns = columns_list)\n",
    "\n",
    "  for epoch_index in range(10):\n",
    "    training_accuracy_unaugmented = []\n",
    "    testing_accuracy_unaugmented = []\n",
    "    testing_fscore_unaugmented = []\n",
    "    training_accuracy_augmented = []\n",
    "    testing_accuracy_augmented = []\n",
    "    testing_fscore_augmented = []\n",
    "\n",
    "    for index in range(epoch_index, len(data_unaugmented), 10):\n",
    "      if (data_unaugmented[index].percentage != df_percentage):\n",
    "        continue\n",
    "\n",
    "      training_accuracy_unaugmented.append(data_unaugmented[index].train_accuracy)\n",
    "      testing_accuracy_unaugmented.append(data_unaugmented[index].test_accuracy)\n",
    "      training_accuracy_augmented.append(data_augmented[index].train_accuracy)\n",
    "      testing_accuracy_augmented.append(data_augmented[index].test_accuracy)\n",
    "      testing_fscore_augmented.append(data_augmented[index].test_metrics[2])\n",
    "      testing_fscore_unaugmented.append(data_unaugmented[index].test_metrics[2])\n",
    "    \n",
    "    training_accuracy_augmented = median(training_accuracy_augmented)\n",
    "    training_accuracy_unaugmented = median(training_accuracy_unaugmented)\n",
    "    testing_accuracy_augmented = median(testing_accuracy_augmented)\n",
    "    testing_accuracy_unaugmented = median(testing_accuracy_unaugmented)\n",
    "    testing_fscore_augmented = median(testing_fscore_augmented)\n",
    "    testing_fscore_unaugmented = median(testing_fscore_unaugmented)\n",
    "\n",
    "    df2 = {'Epoch': epoch_index, \n",
    "           'Augmented_Training_Accuracy': training_accuracy_augmented, \n",
    "           'Augmented_Testing_Accuracy': testing_accuracy_augmented, \n",
    "           'Unaugmented_Training_Accuracy': training_accuracy_unaugmented, \n",
    "           'Unaugmented_Testing_Accuracy': testing_accuracy_unaugmented,\n",
    "           'Augmented_Testing_FScore': testing_fscore_augmented,\n",
    "           'Unaugmented_Testing_FScore': testing_fscore_unaugmented}\n",
    "\n",
    "    data_df = data_df.append(df2, ignore_index = True)\n",
    "\n",
    "  return data_df\n",
    "\n",
    "data_df_10 = get_dataframe(10)\n",
    "data_df_20 = get_dataframe(20)\n",
    "data_df_30 = get_dataframe(30)\n",
    "data_df_40 = get_dataframe(40)\n",
    "data_df_50 = get_dataframe(50)\n",
    "data_df_60 = get_dataframe(60)\n",
    "data_df_70 = get_dataframe(70)\n",
    "data_df_80 = get_dataframe(80)\n",
    "data_df_90 = get_dataframe(90)\n",
    "data_df_100 = get_dataframe(100)\n",
    "\n",
    "data_df_list = [data_df_10, data_df_20, data_df_30, data_df_40, data_df_50, data_df_60, data_df_70, data_df_80, data_df_90, data_df_100]\n",
    "\n",
    "data_df_90\n",
    "\n",
    "count = 10\n",
    "for i in data_df_list:\n",
    "  figure = i.plot(x ='Epoch', y=['Augmented_Testing_FScore', 'Augmented_Testing_Accuracy', 'Augmented_Training_Accuracy', 'Unaugmented_Testing_FScore', 'Unaugmented_Testing_Accuracy', 'Unaugmented_Training_Accuracy'], kind = 'line')\n",
    "  # fig = figure.get_figure()\n",
    "  # fig.savefig(\"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/Plots/Plot_\" + str(count) + '.jpg')\n",
    "  count+=10\n",
    "\n",
    "df2 = pd.DataFrame(columns = ['Percentage', 'Maximum Augmented Accuracy', 'Maximum Unaugmented Accuracy', 'Augmented Weighted FScore', 'Unaugmented Weighted FScore'])\n",
    "for index, df in list(enumerate(data_df_list)):\n",
    "  if (index == 9):\n",
    "    break\n",
    "  print(str((index+1)*10) + \" Percentage:\")\n",
    "  print(str(df[\"Augmented_Testing_Accuracy\"].iloc[-1]) + \", \" + str(df[\"Unaugmented_Testing_Accuracy\"].iloc[-1]))\n",
    "  df2 = df2.append({\n",
    "      'Percentage': (index+1)*10,\n",
    "      'Maximum Augmented Accuracy': round(df[\"Augmented_Testing_Accuracy\"].iloc[-1], 3),\n",
    "      'Maximum Unaugmented Accuracy': round(df[\"Unaugmented_Testing_Accuracy\"].iloc[-1], 3),\n",
    "      'Augmented Weighted FScore': round(df['Augmented_Testing_FScore'].iloc[-1], 3),\n",
    "      'Unaugmented Weighted FScore': round(df['Unaugmented_Testing_FScore'].iloc[-1], 3)\n",
    "  }, ignore_index = True)\n",
    "\n",
    "# df2.to_csv(\"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/results.csv\")\n",
    "\n",
    "df2\n",
    "\n",
    "# human_paths, human_df = get_human_paths()\n",
    "# human_images, human_labels = get_human_images(human_df)\n",
    "# human_images_ct, human_labels_ct = get_human_cartoonized_images(human_ct_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for index, path in list(enumerate(paths)):\n",
    "#   display(ImageOps.grayscale(Image.fromarray(cv2.imread(path))))\n",
    "\n",
    "# file_name = \"data_augmented.pkl\"\n",
    "# file_name2 = \"data_unaugmented.pkl\"\n",
    "# open_file = open(file_name, \"wb\")\n",
    "# pickle.dump(data_augmented, open_file)\n",
    "# open_file.close()\n",
    "# open_file = open(file_name2, \"wb\")\n",
    "# pickle.dump(data_unaugmented, open_file)\n",
    "# open_file.close()\n",
    "\n",
    "\n",
    "\n",
    "# !git clone https://github.com/FilipAndersson245/cartoon-gan\n",
    "\n",
    "# paths = glob.glob(\"/content/drive/MyDrive/Capstone/anime/animated2/Saved_Data/comparison/*.png\")\n",
    "# for index, path in list(enumerate(paths)):\n",
    "#   im1 = np.asarray(ImageOps.grayscale(Image.fromarray(cv2.imread(path)[:, :256])))\n",
    "#   im2 = np.asarray(ImageOps.grayscale(Image.fromarray(cv2.imread(path)[:, 256:])))\n",
    "#   cv2.imwrite(\"/content/drive/MyDrive/Capstone/anime/animated2/Testing/cartoon-gan-master/datasets/human/human_\" + str(index+1) + \".png\", im1)\n",
    "#   cv2.imwrite(\"/content/drive/MyDrive/Capstone/anime/animated2/Testing/cartoon-gan-master/datasets/cartoon/cartoon_\" + str(index+1) + \".png\", im2)\n",
    "\n",
    "\"\"\"# Demo\"\"\"\n",
    "\n",
    "from IPython.display import display, Javascript, clear_output\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename\n",
    "\n",
    "\n",
    "repo = \"CartoonGan-tensorflow\"\n",
    "!git clone https://github.com/mnicnc404/{repo}.git\n",
    "os.chdir(os.path.join(repo))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "model_bottleneck, model_cartoon = get_models()\n",
    "model_demo = load_model(\"/content/drive/MyDrive/Capstone/anime/animated2/demo_model.h5\")\n",
    "model_cartoon = load_model(\"/content/drive/MyDrive/Capstone/anime/animated2/predictor_model\")\n",
    "# model_cartoon = load_model(\"/content/drive/MyDrive/Capstone/anime/animated2/demo_model_combined.h5\")\n",
    "styles = \"shinkai\"\n",
    "key = {0:\"anger\", 1:\"joy\", 2:\"surprise\", 3:\"disgust\", 4:\"fear\", 5:\"sadness\", 6:\"neutral\"}\n",
    "clear_output()\n",
    "\n",
    "def cartoonize_captured():\n",
    "  print(\"PROCESSING....\")\n",
    "\n",
    "  old_stdout = sys.stdout # backup current stdout\n",
    "  sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "  !python cartoonize.py \\\n",
    "      --styles {styles} \\\n",
    "      --batch_size 1 \\\n",
    "      --comparison_view horizontal\n",
    "  \n",
    "  sys.stdout = old_stdout\n",
    "  clear_output()\n",
    "\n",
    "def process_image():\n",
    "  photo_col = np.asarray(Image.fromarray(np.asarray(Image.open('photo.jpg'))[:, 80:560]).resize((256, 256)))\n",
    "  photo = cv2.resize(np.asarray(Image.open('photo.jpg'))[:, 80:560], (256, 256))\n",
    " \n",
    "  new_photo = np.asarray(ImageOps.grayscale(PIL.Image.fromarray(photo)).convert(\"RGB\"))\n",
    "  new_photo = new_photo[:, :, 1].reshape(256, 256)\n",
    "  imagePath = sys.argv[1]\n",
    " \n",
    "  faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "  faces = faceCascade.detectMultiScale(\n",
    "          new_photo,\n",
    "          scaleFactor=1.3,\n",
    "          minNeighbors=3,\n",
    "          minSize=(30, 30)\n",
    "  )\n",
    " \n",
    "  print(\"Found {} Faces!\".format(len(faces)))\n",
    "  for (x, y, w, h) in faces:\n",
    "      rect = cv2.rectangle(photo, (x-10, y-10), (x+w+10, y+h+10), (0, 255, 0), 2)\n",
    "      cv2.rectangle(photo_col, (x-10, y-10), (x+w+10, y+h+10), (0, 255, 0), 2)\n",
    " \n",
    "  crop_photo = cv2.resize(np.asarray(Image.fromarray(photo[y:y+h, x:x+w])), (256, 256))\n",
    "  crop_photo_col = np.asarray(Image.fromarray(photo_col[y:y+h, x:x+w]).resize((256, 256)))\n",
    "  gray = np.asarray(ImageOps.grayscale(Image.fromarray(crop_photo)).convert(\"RGB\"))\n",
    "  gray_col = np.asarray(ImageOps.grayscale(Image.fromarray(crop_photo_col)).convert(\"RGB\"))\n",
    "  pred_photo = np.expand_dims(gray, axis=0)\n",
    "  pred_photo_col = np.expand_dims(gray_col, axis=0)\n",
    " \n",
    "  files1 = glob.glob(\"/content/CartoonGan-tensorflow/input_images/*\")\n",
    "  files2 = glob.glob(\"/content/CartoonGan-tensorflow/output_images/comparison/*\")\n",
    " \n",
    "  for f in files1:\n",
    "      os.remove(f)\n",
    "  for f in files2:\n",
    "      os.remove(f)\n",
    "  if (os.path.exists(\"/content/CartoonGan-tensorflow/output_images/shinkai\")):\n",
    "    files3 = glob.glob(\"/content/CartoonGan-tensorflow/output_images/shinkai/*\")\n",
    "    for f in files3:\n",
    "      os.remove(f)\n",
    "  \n",
    "  Image.fromarray(pred_photo_col[0]).save(\"/content/CartoonGan-tensorflow/input_images/pred_photo_col.jpg\")\n",
    "  Image.fromarray(photo_col).save(\"/content/CartoonGan-tensorflow/input_images/photo_col.jpg\")\n",
    "  \n",
    "  cv2.imwrite(\"/content/CartoonGan-tensorflow/input_images/pred_photo.jpg\", pred_photo[0])\n",
    "  cv2.imwrite(\"/content/CartoonGan-tensorflow/input_images/crop_photo.jpg\", crop_photo)\n",
    " \n",
    "  cartoonize_captured()\n",
    " \n",
    "  cartoon_me = np.asarray(ImageOps.grayscale(Image.open('/content/CartoonGan-tensorflow/output_images/comparison/pred_photo.jpg')).convert(\"RGB\"))[:, 256:]\n",
    "  cartoon_me = np.expand_dims(cartoon_me, axis=0)\n",
    " \n",
    "  pred = model_temp.predict_classes(model_bottleneck.predict(cartoon_me))\n",
    "  prob = model_temp.predict(model_bottleneck.predict(cartoon_me))\n",
    " \n",
    "  if (prob[0][3]>0.5):\n",
    "    preder = 3\n",
    "  else:\n",
    "    temp = prob[0][3]\n",
    "    prob[0][3] = 0\n",
    "    preder = np.argmax(prob[0])\n",
    "    prob[0][3] = temp\n",
    " \n",
    "  # display(Image.fromarray(photo))\n",
    "  display(Image.open('/content/CartoonGan-tensorflow/output_images/comparison/photo_col.jpg'))\n",
    "  display(ImageOps.grayscale(Image.open('/content/CartoonGan-tensorflow/output_images/comparison/pred_photo.jpg')))\n",
    "  print()\n",
    "  print(\"Detected Emotion: \" + str(key[preder]))\n",
    "  print()\n",
    "  print(prob[0])\n",
    "  print()\n",
    "\n",
    "def run_demo():\n",
    "  from IPython.display import Image as imer\n",
    "  try:\n",
    "    filename = take_photo()\n",
    "    print('Saved to {}'.format(filename))\n",
    "    \n",
    "    display(imer(filename))\n",
    "  except Exception as err:\n",
    "\n",
    "    print(str(err))\n",
    "  process_image()\n",
    "\n",
    "run_demo()\n",
    "\n",
    "model_bottleneck, model_cartoon = get_models()\n",
    "human_paths, human_df = get_human_paths()\n",
    "human_images, human_labels = get_human_images(human_df.sample(frac = 1))\n",
    "\n",
    "model_bottleneck, model_human = get_models()\n",
    "model_cartoon = train_human(model_bottleneck, model_cartoon, human_images, human_labels, epochs = 30)\n",
    "model_cartoon = train_human(model_bottleneck, model_cartoon, human_images_cartoon, human_labels_cartoon, epochs = 4)\n",
    "\n",
    "model_cartoon = load_model(\"/content/drive/MyDrive/Capstone/anime/animated2/predictor_model\")\n",
    "\n",
    "model_temp = load_model(\"/content/drive/MyDrive/Capstone/anime/animated2/predictor_model\")\n",
    "\n",
    "predictions, probabilities, accuracy, metrics, confusion = test_human_cartoonized_accuracy(model_bottleneck, model_cartoon, human_df_cartoon, display_image = False)\n",
    "\n",
    "accuracy\n",
    "\n",
    "human_paths_cartoon, human_df_cartoon = get_human_cartoonized_paths()\n",
    "human_images_cartoon, human_labels_cartoon = get_human_cartoonized_images(human_df_cartoon.sample(frac = 1))\n",
    "\n",
    "model_cartoon = train_human(model_bottleneck, model_cartoon, human_images_cartoon, human_labels_cartoon, epochs = 4)\n",
    "\n",
    "predictions, probabilities, accuracy, metrics, confusion = test_human_cartoonized_accuracy(model_bottleneck, model_cartoon, human_df_cartoon, display_image = False)\n",
    "\n",
    "accuracy\n",
    "\n",
    "model_demo = load_model(\"/content/drive/MyDrive/Capstone/anime/animated2/demo_model_combined.h5\")\n",
    "\n",
    "model_cartoon.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = 'accuracy')\n",
    "\n",
    "def process_image():\n",
    "  photo = cv2.resize(np.asarray(Image.open('photo.jpg'))[:, 80:560], (256, 256))\n",
    "  new_photo = np.asarray(ImageOps.grayscale(PIL.Image.fromarray(photo)).convert(\"RGB\"))\n",
    "  new_photo = new_photo[:, :, 1].reshape(256, 256)\n",
    "  imagePath = sys.argv[1]\n",
    "\n",
    "  faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "  faces = faceCascade.detectMultiScale(\n",
    "          photo,\n",
    "          scaleFactor=1.3,\n",
    "          minNeighbors=3,\n",
    "          minSize=(30, 30)\n",
    "  )\n",
    "\n",
    "  print(\"Found {} Faces!\".format(len(faces)))\n",
    "  for (x, y, w, h) in faces:\n",
    "      rect = cv2.rectangle(photo, (x-10, y-10), (x+w+10, y+h+10), (0, 255, 0), 2)\n",
    "\n",
    "  crop_photo = cv2.resize(np.asarray(Image.fromarray(photo[y:y+h, x:x+w])), (256, 256))\n",
    "  gray = np.asarray(ImageOps.grayscale(Image.fromarray(crop_photo)).convert(\"RGB\"))\n",
    "  pred_photo = np.expand_dims(gray, axis=0)\n",
    "\n",
    "  files1 = glob.glob(\"/content/CartoonGan-tensorflow/input_images/*\")\n",
    "  files2 = glob.glob(\"/content/CartoonGan-tensorflow/output_images/comparison/*\")\n",
    "\n",
    "  for f in files1:\n",
    "      os.remove(f)\n",
    "  for f in files2:\n",
    "      os.remove(f)\n",
    "  if (os.path.exists(\"/content/CartoonGan-tensorflow/output_images/shinkai\")):\n",
    "    files3 = glob.glob(\"/content/CartoonGan-tensorflow/output_images/shinkai/*\")\n",
    "    for f in files3:\n",
    "      os.remove(f)\n",
    "      \n",
    "  cv2.imwrite(\"/content/CartoonGan-tensorflow/input_images/pred_photo.jpg\", pred_photo[0])\n",
    "\n",
    "  cartoonize_captured()\n",
    "\n",
    "  cartoon_me = np.asarray(ImageOps.grayscale(Image.open('/content/CartoonGan-tensorflow/output_images/comparison/pred_photo.jpg')).convert(\"RGB\"))[:, 256:]\n",
    "  cartoon_me = np.expand_dims(cartoon_me, axis=0)\n",
    "\n",
    "  pred = model_cartoon.predict_classes(model_bottleneck.predict(cartoon_me))\n",
    "  prob = model_cartoon.predict(model_bottleneck.predict(cartoon_me))\n",
    "\n",
    "  if (prob[0][3]>0.75):\n",
    "    preder = 3\n",
    "  else:\n",
    "    temp = prob[0][3]\n",
    "    prob[0][3] = 0\n",
    "    preder = np.argmax(prob[0])\n",
    "    prob[0][3] = temp\n",
    "\n",
    "  display(ImageOps.grayscale(Image.open('/content/CartoonGan-tensorflow/output_images/comparison/pred_photo.jpg')))\n",
    "  print()\n",
    "  print(\"Detected Emotion: \" + str(key[preder]))\n",
    "  print()\n",
    "  print(prob[0])\n",
    "  print()\n",
    "\n",
    "\"\"\"# Experimental 2\"\"\"\n",
    "\n",
    "temp = glob.glob(\"/content/drive/MyDrive/Capstone/TD_CS_Set1/*/TD_RGB*\")\n",
    "\n",
    "lister = [Image.open(i) for i in temp]\n",
    "\n",
    "files1 = glob.glob(\"/content/CartoonGan-tensorflow/input_images/*\")\n",
    "files2 = glob.glob(\"/content/CartoonGan-tensorflow/output_images/comparison/*\")\n",
    "\n",
    "for f in files1:\n",
    "    os.remove(f)\n",
    "for f in files2:\n",
    "    os.remove(f)\n",
    "if (os.path.exists(\"/content/CartoonGan-tensorflow/output_images/shinkai\")):\n",
    "  files3 = glob.glob(\"/content/CartoonGan-tensorflow/output_images/shinkai/*\")\n",
    "  for f in files3:\n",
    "    os.remove(f)\n",
    "\n",
    "count = 0\n",
    "for i in lister:\n",
    "  i.save(\"/content/CartoonGan-tensorflow/input_images/\" + str(count) + \".jpg\", 'JPEG')\n",
    "  count+=1\n",
    "\n",
    "cartoonize_captured()\n",
    "\n",
    "new_glob = glob.glob(\"/content/CartoonGan-tensorflow/output_images/comparison/*\")\n",
    "\n",
    "new_list = [Image.open(i) for i in new_glob]\n",
    "\n",
    "conc1 = np.asarray(new_list[0])\n",
    "conc2 = np.asarray(new_list[5])\n",
    "conc3 = np.asarray(new_list[10])\n",
    "conc4 = np.asarray(new_list[15])\n",
    "\n",
    "for i in new_list[1:5]:\n",
    "  conc1 = np.hstack([conc1, i])\n",
    "for i in new_list[6:10]:\n",
    "  conc2 = np.hstack([conc2, i])\n",
    "for i in new_list[11:15]:\n",
    "  conc3 = np.hstack([conc3, i])\n",
    "for i in new_list[16:20]:\n",
    "  conc4 = np.hstack([conc4, i])\n",
    "final = np.vstack([conc1, conc2, conc3, conc4])\n",
    "\n",
    "display(Image.fromarray(final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
